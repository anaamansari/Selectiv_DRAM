% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, A.~Rabinovich \emph{et~al.}, ``Going deeper with
  convolutions.''\hskip 1em plus 0.5em minus 0.4em\relax Cvpr, 2015.

\bibitem{Lacey2016}
G.~Lacey, G.~W. Taylor, and S.~Areibi, ``Deep learning on fpgas: Past, present,
  and future,'' \emph{arXiv preprint arXiv:1602.04283}, 2016.

\bibitem{Fowers:2012:PEC:2145694.2145704}
\BIBentryALTinterwordspacing
J.~Fowers, G.~Brown, P.~Cooke, and G.~Stitt, ``A performance and energy
  comparison of fpgas, gpus, and multicores for sliding-window applications,''
  in \emph{Proceedings of the ACM/SIGDA International Symposium on Field
  Programmable Gate Arrays}, ser. FPGA '12.\hskip 1em plus 0.5em minus
  0.4em\relax New York, NY, USA: ACM, 2012, pp. 47--56. [Online]. Available:
  \url{http://doi.acm.org/10.1145/2145694.2145704}
\BIBentrySTDinterwordspacing

\bibitem{sze2017efficient}
V.~Sze, Y.-H. Chen, T.-J. Yang, and J.~S. Emer, ``Efficient processing of deep
  neural networks: A tutorial and survey,'' \emph{Proceedings of the IEEE},
  vol. 105, no.~12, pp. 2295--2329, 2017.

\bibitem{han2016eie}
S.~Han, X.~Liu, H.~Mao, J.~Pu, A.~Pedram, M.~A. Horowitz, and W.~J. Dally,
  ``Eie: efficient inference engine on compressed deep neural network,'' in
  \emph{Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International
  Symposium on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp.
  243--254.

\bibitem{chakradhar2010dynamically}
S.~Chakradhar, M.~Sankaradas, V.~Jakkula, and S.~Cadambi, ``A dynamically
  configurable coprocessor for convolutional neural networks,'' in \emph{ACM
  SIGARCH Computer Architecture News}, vol.~38, no.~3.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM, 2010, pp. 247--257.

\bibitem{zhang2015optimizing}
C.~Zhang, P.~Li, G.~Sun, Y.~Guan, B.~Xiao, and J.~Cong, ``Optimizing fpga-based
  accelerator design for deep convolutional neural networks,'' in
  \emph{Proceedings of the 2015 ACM/SIGDA International Symposium on
  Field-Programmable Gate Arrays}.\hskip 1em plus 0.5em minus 0.4em\relax ACM,
  2015, pp. 161--170.

\bibitem{Eyeriss}
Y.~H. Chen, J.~Emer, and V.~Sze, ``Eyeriss: A spatial architecture for
  energy-efficient dataflow for convolutional neural networks,'' \emph{IEEE
  Micro}, pp. 1--1, 2017.

\bibitem{zhu2017prune}
M.~Zhu and S.~Gupta, ``To prune, or not to prune: exploring the efficacy of
  pruning for model compression,'' \emph{arXiv preprint arXiv:1710.01878},
  2017.

\bibitem{DBLP:journals/corr/abs-1708-04485}
\BIBentryALTinterwordspacing
A.~Parashar, M.~Rhu, A.~Mukkara, A.~Puglielli, R.~Venkatesan, B.~Khailany,
  J.~S. Emer, S.~W. Keckler, and W.~J. Dally, ``{SCNN:} an accelerator for
  compressed-sparse convolutional neural networks,'' \emph{CoRR}, vol.
  abs/1708.04485, 2017. [Online]. Available:
  \url{http://arxiv.org/abs/1708.04485}
\BIBentrySTDinterwordspacing

\bibitem{liu2018efficient}
X.~Liu, J.~Pool, S.~Han, and W.~J. Dally, ``Efficient sparse-winograd
  convolutional neural networks,'' \emph{arXiv preprint arXiv:1802.06367},
  2018.

\bibitem{mao2017exploring}
H.~Mao, S.~Han, J.~Pool, W.~Li, X.~Liu, Y.~Wang, and W.~J. Dally, ``Exploring
  the regularity of sparse structure in convolutional neural networks,''
  \emph{arXiv preprint arXiv:1705.08922}, 2017.

\bibitem{han2016dsd}
S.~Han, J.~Pool, S.~Narang, H.~Mao, S.~Tang, E.~Elsen, B.~Catanzaro, J.~Tran,
  and W.~J. Dally, ``Dsd: Regularizing deep neural networks with
  dense-sparse-dense training flow,'' \emph{arXiv preprint arXiv:1607.04381},
  2016.

\bibitem{han2015deep}
S.~Han, H.~Mao, and W.~J. Dally, ``Deep compression: Compressing deep neural
  networks with pruning, trained quantization and huffman coding,'' \emph{arXiv
  preprint arXiv:1510.00149}, 2015.

\bibitem{li2016pruning}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf, ``Pruning filters for
  efficient convnets,'' \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{wu2016quantized}
J.~Wu, C.~Leng, Y.~Wang, Q.~Hu, and J.~Cheng, ``Quantized convolutional neural
  networks for mobile devices,'' in \emph{Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition}, 2016, pp. 4820--4828.

\bibitem{jouppi2017datacenter}
N.~P. Jouppi, C.~Young, N.~Patil, D.~Patterson, G.~Agrawal, R.~Bajwa, S.~Bates,
  S.~Bhatia, N.~Boden, A.~Borchers \emph{et~al.}, ``In-datacenter performance
  analysis of a tensor processing unit,'' in \emph{Proceedings of the 44th
  Annual International Symposium on Computer Architecture}.\hskip 1em plus
  0.5em minus 0.4em\relax ACM, 2017, pp. 1--12.

\bibitem{HSong_lecture}
\BIBentryALTinterwordspacing
S.~Han, ``Efficient methods and hardware for deep learning,'' University
  Lecture, 2017, May 25. [Online]. Available:
  \url{http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf}
\BIBentrySTDinterwordspacing

\bibitem{kim2016neurocube}
D.~Kim, J.~Kung, S.~Chai, S.~Yalamanchili, and S.~Mukhopadhyay, ``Neurocube: A
  programmable digital neuromorphic architecture with high-density 3d memory,''
  in \emph{Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual
  International Symposium on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2016, pp. 380--392.

\bibitem{gao2017tetris}
\BIBentryALTinterwordspacing
M.~Gao, J.~Pu, X.~Yang, M.~Horowitz, and C.~Kozyrakis, ``Tetris: Scalable and
  efficient neural network acceleration with 3d memory,'' in \emph{Proceedings
  of the Twenty-Second International Conference on Architectural Support for
  Programming Languages and Operating Systems}.\hskip 1em plus 0.5em minus
  0.4em\relax ACM, 2017, pp. 751--764. [Online]. Available:
  \url{https://github.com/stanford-mast/nn_dataflow/}
\BIBentrySTDinterwordspacing

\bibitem{jiang2017xnor}
L.~Jiang, M.~Kim, W.~Wen, and D.~Wang, ``Xnor-pop: A processing-in-memory
  architecture for binary convolutional neural networks in wide-io2 drams,'' in
  \emph{Low Power Electronics and Design (ISLPED, 2017 IEEE/ACM International
  Symposium on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 1--6.

\bibitem{chi2016prime}
P.~Chi, S.~Li, C.~Xu, T.~Zhang, J.~Zhao, Y.~Liu, Y.~Wang, and Y.~Xie, ``Prime:
  a novel processing-in-memory architecture for neural network computation in
  reram-based main memory,'' in \emph{ACM SIGARCH Computer Architecture News},
  vol.~44, no.~3.\hskip 1em plus 0.5em minus 0.4em\relax IEEE Press, 2016, pp.
  27--39.

\bibitem{black2013hybrid}
M.~Black, ``Hybrid memory cube,'' in \emph{Electronic Design Process
  Symposium}, 2013, pp. 3--3.

\bibitem{standard2013high}
J.~Standard, ``High bandwidth memory (hbm) dram,'' \emph{JESD235}, 2013.

\bibitem{alexnet_matlab}
\BIBentryALTinterwordspacing
Matlab. (2018) Pretrained alexnet convolutional neural network. [Online].
  Available: \url{https://www.mathworks.com/help/nnet/ref/alexnet.html}
\BIBentrySTDinterwordspacing

\bibitem{anwar2017structured}
S.~Anwar, K.~Hwang, and W.~Sung, ``Structured pruning of deep convolutional
  neural networks,'' \emph{ACM Journal on Emerging Technologies in Computing
  Systems (JETC)}, vol.~13, no.~3, p.~32, 2017.

\bibitem{Ansari_eff}
A.~Ansari, K.~Gunnam, and T.~Ogunfunmi, ``An efficient reconfigurable hardware
  accelerator for convolutional neural networks,'' in \emph{2017 51st Asilomar
  Conference on Signals, Systems, and Computers}, Oct 2017, pp. 1337--1341.

\bibitem{energy_est_mit}
T.~J. Yang, Y.~H. Chen, J.~Emer, and V.~Sze, ``A method to estimate the energy
  consumption of deep neural networks,'' in \emph{2017 51st Asilomar Conference
  on Signals, Systems, and Computers}, Oct 2017, pp. 1916--1920.

\end{thebibliography}
